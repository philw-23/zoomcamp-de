{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03c6bd65-62c2-4624-8c8d-805b8489ecdc",
   "metadata": {},
   "source": [
    "# Loading Data to Local location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "445bda46-ce2e-46ec-9d6c-494d4013e068",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages needed for data write\n",
    "import requests\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a56534f8-aeaf-4b36-8c58-da87caa2c061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define URLs for data pulls\n",
    "url_taxi = 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2023-01.parquet'\n",
    "url_location = 'https://d37ci6vzurychx.cloudfront.net/misc/taxi_zone_lookup.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9f51c28-8cca-4dda-ae88-8848ffdaae3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output files for data\n",
    "file_taxi = './jan_2023_yellow_tripdata.parquet'\n",
    "file_location = './nyc_taxi_location_zones.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b4fa3128-1533-4e87-9487-6b9229df666e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Define function for writing url data to local file\n",
    "    * url is the url where data is pulled from\n",
    "    * f_name is the location where the data will be written\n",
    "'''\n",
    "def write_data_local(url, f_name):\n",
    "    if not os.path.isfile(f_name): # Check if file already exists!\n",
    "        response = requests.get(url, stream=True) # Stream file with requests to avoid loading all in memory\n",
    "        if response.status_code == 200: # Only continue if we have a successful request\n",
    "            with open(f_name, 'wb') as out: # Open a file to write results to\n",
    "                for resp in response.iter_content(chunk_size=500000): # Stream in 5mb chunks\n",
    "                    if resp: # Handle end cases\n",
    "                        out.write(resp)\n",
    "            print('Data successfully written!')\n",
    "        else:\n",
    "            print(f'Error Encountered! {response.status_code}')\n",
    "    else:\n",
    "        print('File already exists - no need to rewrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "dd43b3dd-eead-4dce-ac77-06e34d57b57d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists - no need to rewrite\n"
     ]
    }
   ],
   "source": [
    "# Execute for taxi data\n",
    "write_data_local(url_taxi, file_taxi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8702f37c-0c24-421d-8f6f-d902e2bf3c97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists - no need to rewrite\n"
     ]
    }
   ],
   "source": [
    "# Execute for location data\n",
    "write_data_local(url_location, file_location)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e491cc-2325-4d7b-9e5e-eed374ef9e82",
   "metadata": {},
   "source": [
    "# Explore Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fcb68dea-4da2-43f2-ad63-8a02c7e44de4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages for data exploration\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a2841255-6c61-4826-9334-4b10014800b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3066766\n"
     ]
    }
   ],
   "source": [
    "# View number of rows in parquet dataset\n",
    "# Because the file is large we do this in chunks to avoid loading everything to memory\n",
    "taxi_pq = pq.ParquetFile(file_taxi) # Open the file\n",
    "num_rows = 0\n",
    "for i in range(taxi_pq.num_row_groups): # iterate over all row groups\n",
    "    row_group = taxi_pq.read_row_group(i) # Load in specific row group\n",
    "    num_rows += row_group.num_rows # Add the number of rows\n",
    "print(num_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f1679eb5-87a1-494d-9db9-6f37716fb5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VendorID: int64\n",
       "tpep_pickup_datetime: timestamp[us]\n",
       "tpep_dropoff_datetime: timestamp[us]\n",
       "passenger_count: double\n",
       "trip_distance: double\n",
       "RatecodeID: double\n",
       "store_and_fwd_flag: string\n",
       "PULocationID: int64\n",
       "DOLocationID: int64\n",
       "payment_type: int64\n",
       "fare_amount: double\n",
       "extra: double\n",
       "mta_tax: double\n",
       "tip_amount: double\n",
       "tolls_amount: double\n",
       "improvement_surcharge: double\n",
       "total_amount: double\n",
       "congestion_surcharge: double\n",
       "airport_fee: double\n",
       "-- schema metadata --\n",
       "pandas: '{\"index_columns\": [], \"column_indexes\": [], \"columns\": [{\"name\":' + 2492"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can also view the schema information on the parquet file!\n",
    "taxi_data = next(taxi_pq.iter_batches(batch_size=100000))\n",
    "taxi_data.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "53855a1c-6f9f-433a-84b1-bccf281ab497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 19 columns):\n",
      " #   Column                 Non-Null Count   Dtype         \n",
      "---  ------                 --------------   -----         \n",
      " 0   VendorID               100000 non-null  int64         \n",
      " 1   tpep_pickup_datetime   100000 non-null  datetime64[us]\n",
      " 2   tpep_dropoff_datetime  100000 non-null  datetime64[us]\n",
      " 3   passenger_count        100000 non-null  float64       \n",
      " 4   trip_distance          100000 non-null  float64       \n",
      " 5   RatecodeID             100000 non-null  float64       \n",
      " 6   store_and_fwd_flag     100000 non-null  object        \n",
      " 7   PULocationID           100000 non-null  int64         \n",
      " 8   DOLocationID           100000 non-null  int64         \n",
      " 9   payment_type           100000 non-null  int64         \n",
      " 10  fare_amount            100000 non-null  float64       \n",
      " 11  extra                  100000 non-null  float64       \n",
      " 12  mta_tax                100000 non-null  float64       \n",
      " 13  tip_amount             100000 non-null  float64       \n",
      " 14  tolls_amount           100000 non-null  float64       \n",
      " 15  improvement_surcharge  100000 non-null  float64       \n",
      " 16  total_amount           100000 non-null  float64       \n",
      " 17  congestion_surcharge   100000 non-null  float64       \n",
      " 18  airport_fee            100000 non-null  float64       \n",
      "dtypes: datetime64[us](2), float64(12), int64(4), object(1)\n",
      "memory usage: 14.5+ MB\n"
     ]
    }
   ],
   "source": [
    "# Convert to pandas and check data\n",
    "taxi_df = taxi_data.to_pandas() # Converts parquet table to pandas\n",
    "taxi_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c22865-36ea-4b69-a697-453792a4e1c0",
   "metadata": {},
   "source": [
    "# Writing data to Postgres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "6de0ad28-de21-4be8-8a94-0303e9c6e62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages needed\n",
    "import time # For timing purposes!\n",
    "from sqlalchemy import URL, create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "50cd40e2-0135-4978-bc99-048537b20e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL and engine object\n",
    "pg_URL = URL.create(\n",
    "    drivername='postgresql', # Driver to use\n",
    "    username='root', # Username of login\n",
    "    password='root', # Password of login\n",
    "    host='localhost', # Host for login\n",
    "    port='5432', # Connection port\n",
    "    database='ny_taxi' # Database for connection\n",
    ")\n",
    "# Note: if the below fails, you may need to install psycopg2-binary via pip in your virtual env\n",
    "pg_engine = create_engine(pg_URL) # Create URL (NOTE: postgres container must be active for this to work!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "adf00ce1-0cf8-4631-b71c-dbcf02dd1396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CREATE TABLE yellow_taxi_data (\n",
      "\t\"VendorID\" BIGINT, \n",
      "\ttpep_pickup_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\ttpep_dropoff_datetime TIMESTAMP WITHOUT TIME ZONE, \n",
      "\tpassenger_count FLOAT(53), \n",
      "\ttrip_distance FLOAT(53), \n",
      "\t\"RatecodeID\" FLOAT(53), \n",
      "\tstore_and_fwd_flag TEXT, \n",
      "\t\"PULocationID\" BIGINT, \n",
      "\t\"DOLocationID\" BIGINT, \n",
      "\tpayment_type BIGINT, \n",
      "\tfare_amount FLOAT(53), \n",
      "\textra FLOAT(53), \n",
      "\tmta_tax FLOAT(53), \n",
      "\ttip_amount FLOAT(53), \n",
      "\ttolls_amount FLOAT(53), \n",
      "\timprovement_surcharge FLOAT(53), \n",
      "\ttotal_amount FLOAT(53), \n",
      "\tcongestion_surcharge FLOAT(53), \n",
      "\tairport_fee FLOAT(53)\n",
      ")\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View create table statement that will be ran\n",
    "table_name = 'yellow_taxi_data' # Name to use for the table in the database\n",
    "print(pd.io.sql.get_schema(taxi_df, name=table_name, con=pg_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "86f83128-14f4-4e1c-9358-97c05df6694d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing batch 1\n",
      "Batch 1 completed in 9.108 seconds\n",
      "Writing batch 2\n",
      "Batch 2 completed in 8.484 seconds\n",
      "Writing batch 3\n",
      "Batch 3 completed in 7.641 seconds\n",
      "Writing batch 4\n",
      "Batch 4 completed in 7.399 seconds\n",
      "Writing batch 5\n",
      "Batch 5 completed in 7.924 seconds\n",
      "Writing batch 6\n",
      "Batch 6 completed in 8.401 seconds\n",
      "Writing batch 7\n",
      "Batch 7 completed in 8.247 seconds\n",
      "Writing batch 8\n",
      "Batch 8 completed in 8.219 seconds\n",
      "Writing batch 9\n",
      "Batch 9 completed in 7.576 seconds\n",
      "Writing batch 10\n",
      "Batch 10 completed in 7.265 seconds\n",
      "Writing batch 11\n",
      "Batch 11 completed in 8.383 seconds\n",
      "Writing batch 12\n",
      "Batch 12 completed in 7.606 seconds\n",
      "Writing batch 13\n",
      "Batch 13 completed in 8.268 seconds\n",
      "Writing batch 14\n",
      "Batch 14 completed in 7.806 seconds\n",
      "Writing batch 15\n",
      "Batch 15 completed in 7.647 seconds\n",
      "Writing batch 16\n",
      "Batch 16 completed in 7.869 seconds\n",
      "Writing batch 17\n",
      "Batch 17 completed in 7.682 seconds\n",
      "Writing batch 18\n",
      "Batch 18 completed in 6.842 seconds\n",
      "Writing batch 19\n",
      "Batch 19 completed in 7.444 seconds\n",
      "Writing batch 20\n",
      "Batch 20 completed in 7.151 seconds\n",
      "Writing batch 21\n",
      "Batch 21 completed in 7.142 seconds\n",
      "Writing batch 22\n",
      "Batch 22 completed in 7.691 seconds\n",
      "Writing batch 23\n",
      "Batch 23 completed in 7.952 seconds\n",
      "Writing batch 24\n",
      "Batch 24 completed in 7.187 seconds\n",
      "Writing batch 25\n",
      "Batch 25 completed in 7.767 seconds\n",
      "Writing batch 26\n",
      "Batch 26 completed in 15.306 seconds\n",
      "Writing batch 27\n",
      "Batch 27 completed in 8.173 seconds\n",
      "Writing batch 28\n",
      "Batch 28 completed in 9.415 seconds\n",
      "Writing batch 29\n",
      "Batch 29 completed in 7.465 seconds\n",
      "Writing batch 30\n",
      "Batch 30 completed in 12.307 seconds\n",
      "Writing batch 31\n",
      "Batch 31 completed in 4.489 seconds\n",
      "Completed write in 252.931 seconds\n"
     ]
    }
   ],
   "source": [
    "# Iterate through chunks to write data\n",
    "init_time = time.time() # Total start time\n",
    "\n",
    "# Iterate through in 100K sized batches as file is too large for memory\n",
    "with pg_engine.connect() as conn:\n",
    "    for idx, batch in enumerate(taxi_pq.iter_batches(batch_size=100000)): \n",
    "        # idx is the index of the batch, batch is the data to writeython -m pip install psycopg2-binary\n",
    "        batch_df = batch.to_pandas() # Convert to pandas df\n",
    "        if idx == 0: # On first baython -m pip install psycopg2-binary, create table and replace if it exists\n",
    "            batch_df.head(0).to_sql(name=table_name, con=conn, \n",
    "                                    if_exists='replace', index=False) # Write only header, clear data present\n",
    "        print(f'Writing batch {str(idx + 1)}')\n",
    "        start_time = time.time() # Start time of write\n",
    "        batch_df.to_sql(name=table_name, con=conn, if_exists='append', index=False) # Write batch and append to result\n",
    "        total_time = time.time() - start_time # Time to write batch\n",
    "        print(f'Batch {str(idx + 1)} completed in {str(round(total_time, 3))} seconds')\n",
    "    \n",
    "    # Write out total completion time\n",
    "    print(f'Completed write in {str(round(time.time() - init_time, 3))} seconds')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "34684604-f7ea-4ca3-af6c-d7bfac9f7de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The other file is much smaller and can be handled with pure pandas\n",
    "taxi_zones = pd.read_csv(file_location)\n",
    "with pg_engine.connect() as conn:\n",
    "    taxi_zones.to_sql(name='pickup_locations', con=conn, if_exists='replace', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "de24cfda-d81d-42c0-a122-6f53825b42e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Close engine once completed\n",
    "pg_engine.dispose()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
